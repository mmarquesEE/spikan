{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs detectadas: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "TensorFlow está utilizando GPU:\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 4189278737756327976\n",
      "xla_global_id: -1\n",
      "\n",
      "name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 1722023936\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 9207829409868402409\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 3050 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\"\n",
      "xla_global_id: 416903419\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Verifica se há GPUs disponíveis\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"GPUs detectadas: {gpus}\")\n",
    "    # Testa se a GPU está realmente sendo usada\n",
    "    print(\"TensorFlow está utilizando GPU:\")\n",
    "    print(tf.config.list_physical_devices('GPU'))\n",
    "else:\n",
    "    print(\"Nenhuma GPU foi detectada.\")\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "# Lista todos os dispositivos disponíveis\n",
    "devices = device_lib.list_local_devices()\n",
    "for device in devices:\n",
    "    print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# System Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def f(t,x,K):\n",
    "    if isinstance(x, tf.Tensor):\n",
    "        e = x[...,0]\n",
    "        a = x[...,1]\n",
    "        o = x[...,2]\n",
    "\n",
    "        g = K[...,0]\n",
    "        k = K[...,1]\n",
    "        h = K[...,2]\n",
    "\n",
    "        ca = tf.cos(a)\n",
    "        sa = tf.sin(a)\n",
    "\n",
    "    else:\n",
    "        e, a, o = x\n",
    "        g, k, h = K\n",
    "        ca = np.cos(a)\n",
    "        sa = np.sin(a)\n",
    "    \n",
    "    de = -g*e*ca**2.\n",
    "    da = -g*h*o/a*ca*sa - k*a\n",
    "    do = g*ca*sa\n",
    "\n",
    "    if isinstance(x, tf.Tensor):\n",
    "        return tf.stack([de,da,do],axis=-1)\n",
    "    else:\n",
    "        return [de,da,do]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polar_to_cartesian(y,yG):\n",
    "    xG = yG[...,0:2]\n",
    "    phiG = yG[...,-1]\n",
    "\n",
    "    phi = y[...,2] - y[...,1] + phiG\n",
    "    xG = np.array([[0., 0.]])\n",
    "    x = xG - tf.expand_dims(y[...,0],axis=-1)*tf.stack([tf.cos(y[...,1]),tf.sin(y[...,1])],axis=-1)\n",
    "\n",
    "    return tf.concat([x,tf.expand_dims(phi,axis=-1)],axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trajectory Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import solve_ivp\n",
    "\n",
    "def generate_trajectory(Tf,Ts,x0,K):\n",
    "    teval = np.linspace(0., Tf, int(Tf/Ts))\n",
    "    sol = solve_ivp(f, [0., Tf], x0, args=[K], t_eval=teval)\n",
    "    return sol.t.T, sol.y.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(Tf, Ts, N):\n",
    "    umax = 1.\n",
    "    wmax = 2.4\n",
    "    emax = 5.\n",
    "    \n",
    "    hmax = 1.\n",
    "    gmax = umax/emax\n",
    "    kmax = (wmax - gmax/2*(1+hmax))/np.pi\n",
    "\n",
    "\n",
    "    e0 = np.random.uniform(1e-3,emax,N)\n",
    "    a0 = np.random.uniform(-(np.pi + 1e-3),np.pi + 1e-3,N)\n",
    "    o0 = np.random.uniform(-(np.pi + 1e-3),np.pi + 1e-3,N)\n",
    "\n",
    "    g = np.random.uniform(1e-3*gmax,gmax,N)\n",
    "    k = np.random.uniform(1e-3*kmax,kmax,N)\n",
    "    h = np.random.uniform(1e-3*hmax,hmax,N)\n",
    "\n",
    "    T = []\n",
    "    X = []\n",
    "    K = []\n",
    "    for i in range(N):\n",
    "        t, y = generate_trajectory(\n",
    "            Tf,Ts,\n",
    "            [e0[i],a0[i],o0[i]],\n",
    "            [g[i],k[i],h[i]]\n",
    "        )\n",
    "        T.append(t)\n",
    "        X.append(y)\n",
    "        K.append(np.repeat(np.array([[g[i],k[i],h[i]]]),len(t),axis=0))\n",
    "        \n",
    "    return  tf.cast(tf.expand_dims((tf.stack(T,axis=0)),axis=-1),dtype=tf.float32), \\\n",
    "            tf.cast(tf.stack(X,axis=0),dtype=tf.float32), \\\n",
    "            tf.cast(tf.stack(K,axis=0),dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 200, 1)\n",
      "(1000, 200, 3)\n",
      "(1000, 200, 3)\n"
     ]
    }
   ],
   "source": [
    "N = 1000\n",
    "T, X, U = generate_dataset(10.,50e-3,N)\n",
    "\n",
    "print(T.shape)\n",
    "print(X.shape)\n",
    "print(U.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 200, 1)\n",
      "(800, 200, 3)\n",
      "(800, 200, 3)\n",
      "(100, 200, 1)\n",
      "(100, 200, 3)\n",
      "(100, 200, 3)\n",
      "(100, 200, 1)\n",
      "(100, 200, 3)\n",
      "(100, 200, 3)\n"
     ]
    }
   ],
   "source": [
    "train_idx = int(0.8*N)\n",
    "val_idx = train_idx + int(0.1*N)\n",
    "\n",
    "t = T[:train_idx]\n",
    "x = X[:train_idx]\n",
    "u = U[:train_idx]\n",
    "\n",
    "tv = T[train_idx:val_idx]\n",
    "xv = X[train_idx:val_idx]\n",
    "uv = U[train_idx:val_idx]\n",
    "\n",
    "tt = T[val_idx:]\n",
    "xt = X[val_idx:]\n",
    "ut = U[val_idx:]\n",
    "\n",
    "print(t.shape)\n",
    "print(x.shape)\n",
    "print(u.shape)\n",
    "\n",
    "print(tv.shape)\n",
    "print(xv.shape)\n",
    "print(uv.shape)\n",
    "\n",
    "print(tt.shape)\n",
    "print(xt.shape)\n",
    "print(ut.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class KANLinear(tf.keras.layers.Layer):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(KANLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        self.scale = self.add_weight(\n",
    "            shape=(out_features, in_features), \n",
    "            initializer=tf.keras.initializers.lecun_normal(seed=42)\n",
    "        )\n",
    "        \n",
    "        self.translation = self.add_weight(\n",
    "            shape=(out_features, in_features), \n",
    "            initializer=tf.keras.initializers.lecun_normal(seed=42)\n",
    "        )\n",
    "        \n",
    "        self.wavelet_weights = self.add_weight(\n",
    "            shape=(out_features, in_features),\n",
    "            initializer=tf.keras.initializers.lecun_normal(seed=42)\n",
    "        )\n",
    "\n",
    "        # Batch normalization\n",
    "        # self.bn = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    def wavelet_transform(self, y, *args):\n",
    "        # x is assumed to be (n_batch, sequence_length, n_inputs)\n",
    "        y_scaled = (tf.expand_dims(y, axis=-2) - (self.translation)) / (self.scale + 1e-3)  # Broadcasting handles dimension alignment\n",
    "\n",
    "        # Apply the wavelet transformation\n",
    "        term1 = (y_scaled ** 2) # - 1\n",
    "        term2 = tf.exp(-(y_scaled ** 2))#*0.5)\n",
    "        wavelet = term1 * term2 #* (2 / (math.sqrt(3) * math.pi**0.25))\n",
    "\n",
    "        # Apply weights and sum along the input features dimension\n",
    "        wavelet_weighted = wavelet * (self.wavelet_weights)\n",
    "        wavelet_output = tf.reduce_sum(wavelet_weighted, axis=-1)  # Resulting shape: (n_batch, sequence_length, out_features)\n",
    "\n",
    "        return wavelet_output\n",
    "\n",
    "    def call(self, y, *args):\n",
    "        wavelet_output = self.wavelet_transform(y, *args)\n",
    "        \n",
    "        # Apply batch normalization\n",
    "        # return self.bn(wavelet_output)\n",
    "        return wavelet_output\n",
    "    \n",
    "class FastKAN(tf.keras.Model):\n",
    "    def __init__(self, layers_hidden):\n",
    "        super(FastKAN, self).__init__()\n",
    "        self.layers_list = []\n",
    "\n",
    "        for in_dim, out_dim in zip(layers_hidden[:-1], layers_hidden[1:]):\n",
    "            self.layers_list.append(KANLinear(in_dim, out_dim))\n",
    "\n",
    "    def call(self, t, *args):\n",
    "        y = tf.concat([t,*args],axis=-1)\n",
    "\n",
    "        for layer in self.layers_list:\n",
    "            y = layer(y)\n",
    "        return y\n",
    "\n",
    "class SeperableKAN(tf.keras.Model):\n",
    "    def __init__(self, nin, nout, layers_hidden):\n",
    "        super(SeperableKAN, self).__init__()\n",
    "        self.kans_list = []\n",
    "        self.reshape_list = []\n",
    "        for i in range(nin):\n",
    "            self.kans_list.append(FastKAN([1] + layers_hidden[1:-1] + [nout*layers_hidden[-1]]))\n",
    "            self.reshape_list.append(tf.keras.layers.Reshape((-1,nout,layers_hidden[-1])))\n",
    "        \n",
    "    def call(self, t, *args):\n",
    "        y = tf.concat([t,*args],axis=-1)\n",
    "\n",
    "        temp = 1.0\n",
    "        for i, l in enumerate(zip(self.kans_list,self.reshape_list)):\n",
    "            kan, reshape = l\n",
    "            temp = temp*reshape(kan(y[...,i]))\n",
    "        \n",
    "        return tf.reduce_sum(temp,axis=-1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PINN Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(t, x, u0, model):\n",
    "    x0 = tf.repeat(tf.expand_dims(x[:,0,:], axis=1), repeats=t.shape[1], axis=1)\n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as tape1:\n",
    "        tape1.watch([t, x0, u0])\n",
    "        pred = model(t, x0, u0)\n",
    "        \n",
    "        X = [pred[...,i] for i in range(pred.shape[-1])]\n",
    "        DXDT = [tape1.gradient(xi,t) for xi in X]\n",
    "    \n",
    "    dxdt = tf.concat(DXDT, axis=-1)\n",
    "\n",
    "    err = (x - pred)/tf.abs(x)*100.\n",
    "    err_d = (f(t,x,u0) - dxdt)/tf.abs(f(t,x,u0))*100.\n",
    "\n",
    "    L_ic = tf.reduce_mean(tf.norm(err[:,0,:], axis=-1))\n",
    "    L_bc = tf.reduce_mean(tf.norm(err, axis=-1))\n",
    "    L_r1 = tf.reduce_mean(tf.norm(err_d, axis=-1))\n",
    "\n",
    "    return L_ic, L_bc, L_r1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "model = SeperableKAN(7,3,[6,6])\n",
    "optimizer = optimizers.Adam(learning_rate=1.e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(t, x, u, tv, xv, uv, model, lambdas, reweight=False):\n",
    "    lambda_ic, lambda_bc, lambda_r1 = lambdas\n",
    "    lambdas_new = lambdas\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch([t] + model.trainable_variables)\n",
    "        with tf.GradientTape(persistent=True) as tape1:\n",
    "            tape1.watch([t] + model.trainable_variables)\n",
    "            L_ic, L_bc, L_r1 = loss_function(t, x, u, model)\n",
    "            L_ic_v, L_bc_v, L_r1_v = loss_function(tv, xv, uv, model)\n",
    "\n",
    "            L = lambda_ic*L_ic + lambda_bc*L_bc + lambda_r1*L_r1 \n",
    "            L_v = lambda_ic*L_ic_v + lambda_bc*L_bc_v + lambda_r1*L_r1_v\n",
    "\n",
    "        gradients = tape1.gradient(L,  model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "        \n",
    "    if reweight == True:\n",
    "        grad_L_ic = tape.gradient(L_ic, model.trainable_variables)\n",
    "        grad_L_bc = tape.gradient(L_bc, model.trainable_variables)\n",
    "        grad_L_r1 = tape.gradient(L_r1, model.trainable_variables)\n",
    "\n",
    "        norm_grad_L_ic = tf.sqrt(sum([tf.reduce_mean(tf.square(g)) for g in grad_L_ic if g is not None]))\n",
    "        norm_grad_L_bc = tf.sqrt(sum([tf.reduce_mean(tf.square(g)) for g in grad_L_bc if g is not None]))\n",
    "        norm_grad_L_r1 = tf.sqrt(sum([tf.reduce_mean(tf.square(g)) for g in grad_L_r1 if g is not None]))\n",
    "        \n",
    "        norm_grad_total = sum([norm_grad_L_ic, norm_grad_L_bc, norm_grad_L_r1])\n",
    "\n",
    "        lambda_ic = norm_grad_total/norm_grad_L_ic\n",
    "        lambda_bc = norm_grad_total/norm_grad_L_bc\n",
    "        lambda_r1 = norm_grad_total/norm_grad_L_r1\n",
    "    \n",
    "    lambdas_new = [lambda_ic, lambda_bc, lambda_r1]\n",
    "\n",
    "    # lambda_max = tf.reduce_max(tf.stack(lambdas_new))\n",
    "    # lambdas_new = [l/lambda_max for l in lambdas_new]\n",
    "\n",
    "    return L, L_v, [L_ic, L_bc, L_r1], [L_ic_v, L_bc_v, L_r1_v], lambdas_new\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = [tf.constant(el) for el in [1.,1.,1.]]\n",
    "alpha = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from math import ceil\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "s_reweight = 10000\n",
    "partitions = 4.\n",
    "verbose = 100\n",
    "\n",
    "start_time = time.time()\n",
    "losses = []\n",
    "total_time = 0\n",
    "\n",
    "cont = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = 300000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[166], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m reweight\u001b[38;5;241m=\u001b[39m(((cont \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m s_reweight \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)) \u001b[38;5;241m|\u001b[39m (cont \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# reweight=(cont == 0)\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m li, l_vi, Li, L_vi, lambdas_new \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mti\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mui\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambdas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreweight\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m li\u001b[38;5;241m/\u001b[39mpartitions\n\u001b[0;32m     28\u001b[0m loss_v \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m l_vi\u001b[38;5;241m/\u001b[39mpartitions\n",
      "File \u001b[1;32mc:\\Users\\mmarq\\OneDrive\\Desktop\\Mestrado\\Semestre1\\ProcessamentoAdaptativoDeSinais\\spikan\\.conda\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\mmarq\\OneDrive\\Desktop\\Mestrado\\Semestre1\\ProcessamentoAdaptativoDeSinais\\spikan\\.conda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\mmarq\\OneDrive\\Desktop\\Mestrado\\Semestre1\\ProcessamentoAdaptativoDeSinais\\spikan\\.conda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\mmarq\\OneDrive\\Desktop\\Mestrado\\Semestre1\\ProcessamentoAdaptativoDeSinais\\spikan\\.conda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mmarq\\OneDrive\\Desktop\\Mestrado\\Semestre1\\ProcessamentoAdaptativoDeSinais\\spikan\\.conda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\mmarq\\OneDrive\\Desktop\\Mestrado\\Semestre1\\ProcessamentoAdaptativoDeSinais\\spikan\\.conda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\mmarq\\OneDrive\\Desktop\\Mestrado\\Semestre1\\ProcessamentoAdaptativoDeSinais\\spikan\\.conda\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while cont <= S:\n",
    "    idx_t = np.linspace(0,len(t)-1,len(t),dtype=np.int32)\n",
    "    \n",
    "    loss = 0\n",
    "    loss_v = 0\n",
    "    L = [0.,0.,0.]\n",
    "    L_v = [0.,0.,0.]\n",
    "\n",
    "    for j in range(int(partitions)):\n",
    "        idx_min = j*ceil(len(t)/partitions)\n",
    "        idx_max = (j+1)*ceil(len(t)/partitions)\n",
    "        \n",
    "        if not idx_max < len(t):\n",
    "            idx_max = len(t)\n",
    "        \n",
    "        ti = t[idx_min:idx_max,...]\n",
    "        xi = x[idx_min:idx_max,...]\n",
    "        ui = u[idx_min:idx_max,...]\n",
    "\n",
    "        reweight=(((cont + 1) % s_reweight == 0)) | (cont == 0)\n",
    "        # reweight=(cont == 0)\n",
    "\n",
    "        li, l_vi, Li, L_vi, lambdas_new = train_step(\n",
    "            ti, xi, ui, tv, xv, uv, \n",
    "            model, lambdas, reweight=reweight\n",
    "        )\n",
    "        loss += li/partitions\n",
    "        loss_v += l_vi/partitions\n",
    "        L   = [l + e/partitions for (l,e) in zip(L,Li)]\n",
    "        L_v = [l + e/partitions for (l,e) in zip(L_v,L_vi)]\n",
    "\n",
    "        if reweight:\n",
    "            if cont == 0:\n",
    "                lambdas = [l_n for l_n in lambdas_new]\n",
    "            else:\n",
    "                lambdas = [alpha*l + (1.0-alpha)*l_n for (l,l_n) in zip(lambdas,lambdas_new)]\n",
    "\n",
    "    losses.append([loss, loss_v] + L + L_v)\n",
    "\n",
    "    if (cont + 1) % verbose == 0:\n",
    "        print(f\"Epoch: {cont + 1}\\nLoss: {loss}\\nL: {[l.numpy() for l in L]}\\nlambdas: {[l.numpy() for l in lambdas]}\")\n",
    "        print(f\"Validation: {loss_v}\\nL_v: {[l.numpy() for l in L_v]}\")\n",
    "        elapsed_time = time.time() - start_time\n",
    "        total_time += elapsed_time\n",
    "        print(f\"Elapsed time: {elapsed_time}\\n\")\n",
    "        start_time = time.time()\n",
    "    \n",
    "    cont += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_time/3600.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "losses_tf = tf.reshape(losses, [len(losses),-1])\n",
    "\n",
    "# Criar gráficos para delta e omega\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(211)\n",
    "plt.plot(np.log10(losses_tf[::1000,0:2]), label='')\n",
    "plt.subplot(212)\n",
    "plt.plot(np.log10(losses_tf[::1000,2:]))\n",
    "plt.xlabel('Tempo (s)')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escolha uma trajetória aleatória para comparar\n",
    "print(index)\n",
    "\n",
    "t_s = tt[index,...]\n",
    "t_i = tf.expand_dims(t_s,axis=0)\n",
    "reps = t_i.shape[1]\n",
    "x_s = xt[index]\n",
    "x0_s = tf.expand_dims(tf.tile(tf.expand_dims(xt[index,0,:],axis=0),(reps,1)),axis=0)\n",
    "ul_s = tf.expand_dims(tf.tile(tf.expand_dims(ut[index],axis=0),(reps,1)),axis=0)\n",
    "\n",
    "y = model(t_i,x0_s,ul_s)\n",
    "\n",
    "# Criar gráficos para delta e omega\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(t[index], x_s[:, 0], label='Delta (Dados de Treinamento)')\n",
    "plt.plot(t[index], y[0,:, 0], label='Delta (Previsão)')\n",
    "plt.xlabel('Tempo (s)')\n",
    "plt.ylabel('Delta')\n",
    "plt.title('Comparação Delta')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(t_s, x_s[:, 1], label='Omega (Dados de Treinamento)')\n",
    "plt.plot(t_s, y[0,:, 1], label='Omega (Previsão)')\n",
    "plt.xlabel('Tempo (s)')\n",
    "plt.ylabel('Omega')\n",
    "plt.title('Comparação Omega')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "index = index + 1\n",
    "if index >= len(tv):\n",
    "    index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_tf = tf.constant(tt)\n",
    "xt_tf = tf.constant(xt)\n",
    "ut_tf = tf.constant(ut)\n",
    "loss_t = loss_function(tt_tf,xt_tf,ut_tf,model)\n",
    "\n",
    "print([l.numpy() for l in loss_t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "xi = tf.expand_dims(xt[:,0,:],axis=1)\n",
    "print(len(tt[0,:,0]))\n",
    "print(xi.shape)\n",
    "for i in range(10):\n",
    "\n",
    "    pred = model(tt,tf.repeat(xi,len(tt[0,:,0]),axis=1),tf.repeat(tf.expand_dims(ut,axis=1),len(tt[0,:,0]),axis=1))\n",
    "    predictions.append(pred)\n",
    "    xi = tf.expand_dims(pred[:,-1,:],axis=1)\n",
    "\n",
    "X = tf.concat(predictions,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(index)\n",
    "\n",
    "predictions = []\n",
    "xi = tf.expand_dims(xt[:,0,:],axis=1)\n",
    "print(len(tt[0,:,0]))\n",
    "print(xi.shape)\n",
    "for i in range(10):\n",
    "\n",
    "    pred = model(tt,tf.repeat(xi,len(tt[0,:,0]),axis=1),tf.repeat(tf.expand_dims(ut,axis=1),len(tt[0,:,0]),axis=1))\n",
    "    predictions.append(pred)\n",
    "    xi = tf.expand_dims(pred[:,-1,:],axis=1)\n",
    "\n",
    "X = tf.concat(predictions,axis=1)\n",
    "\n",
    "tRef, Xref = generate_trajectory(1., 1e-3, xt[index,0,:], u[index,:])\n",
    "\n",
    "# Criar gráficos para delta e omega\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(tRef, Xref[:, 0], label='Delta (Dados de Treinamento)')\n",
    "plt.plot(tRef, X[index,:,0], label='Delta (Previsão)')\n",
    "plt.xlabel('Tempo (s)')\n",
    "plt.ylabel('Delta')\n",
    "plt.title('Comparação Delta')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(tRef, Xref[:, 1], label='Omega (Dados de Treinamento)')\n",
    "plt.plot(tRef, X[index,:,1], label='Omega (Previsão)')\n",
    "plt.xlabel('Tempo (s)')\n",
    "plt.ylabel('Omega')\n",
    "plt.title('Comparação Omega')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "index = index + 1\n",
    "if index >= len(xt[:,0,0]):\n",
    "    index = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
